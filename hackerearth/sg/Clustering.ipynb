{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem2-perfect.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rw-pFQ_JRIbf",
        "colab_type": "code",
        "outputId": "8134c121-c719-4a88-aded-15065f9b3c5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zNcWvdI7RO8i",
        "colab_type": "code",
        "outputId": "65e098bf-8ddf-4c80-8377-549793d6a332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "ls 'gdrive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Complaint Status Tracking.ipynb'   prediction2.txt       Problem2.ipynb\n",
            " model.h5                           prediction.csv\n",
            " prediction2.csv                   'Problem2 (1).ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6L_m4hNRS1y",
        "colab_type": "code",
        "outputId": "ec52fb8a-dcd8-4ec2-e8b6-a2275b592a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vikashvverma/ml-contests.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ml-contests' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "djBNhu3xRZDS",
        "colab_type": "code",
        "outputId": "4917ed54-0e93-4a4f-bc4d-a00ee941c1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ls ml-contests/hackerearth/sg/dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "news.csv  sample_submission.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XWwjI_biRfIy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = \"ml-contests/hackerearth/sg/dataset\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jWhug3ewQwtr",
        "colab_type": "code",
        "outputId": "a2526899-d00d-4fee-a018-c1b5974793b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install stemming\n",
        "# !pip install StringIO"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stemming in /usr/local/lib/python3.6/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nFz3I9Y4PewR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zo0e053RQG3Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('{}/news.csv'.format(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H_awqInaRmFU",
        "colab_type": "code",
        "outputId": "1b0f3d06-f40c-4857-da3c-532c35b150b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>headline</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>uid-1</td>\n",
              "      <td>Market Advances 5.12 More</td>\n",
              "      <td>NEW YORK (AP) - A prime rate reduction by New ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>uid-2</td>\n",
              "      <td>District Boosts Request For Anti-Terrorism Aid...</td>\n",
              "      <td>Mayor Anthony A. Williams petitioned the White...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uid-3</td>\n",
              "      <td>Election? Here's How You Do It, Mate.</td>\n",
              "      <td>From our downunder perspective here in Austral...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>uid-4</td>\n",
              "      <td>The Biggest Boom Ever</td>\n",
              "      <td>We are about to rewrite history. Unless a rece...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>uid-5</td>\n",
              "      <td>Economic Aide Sees Uptrend</td>\n",
              "      <td>Sedate and scholarly Dr. Arthur Burns, the ex-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                           headline  \\\n",
              "0  uid-1                          Market Advances 5.12 More   \n",
              "1  uid-2  District Boosts Request For Anti-Terrorism Aid...   \n",
              "2  uid-3              Election? Here's How You Do It, Mate.   \n",
              "3  uid-4                              The Biggest Boom Ever   \n",
              "4  uid-5                         Economic Aide Sees Uptrend   \n",
              "\n",
              "                                                text  \n",
              "0  NEW YORK (AP) - A prime rate reduction by New ...  \n",
              "1  Mayor Anthony A. Williams petitioned the White...  \n",
              "2  From our downunder perspective here in Austral...  \n",
              "3  We are about to rewrite history. Unless a rece...  \n",
              "4  Sedate and scholarly Dr. Arthur Burns, the ex-...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "StCrov2WRn7Y",
        "colab_type": "code",
        "outputId": "bfbd6e4c-18c8-47ef-f47f-7539f0642e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "7BDofBUvSt0d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = train_data.fillna('NULL')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88Kzs9QJJaCj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WylO3ph-VhVX",
        "colab_type": "code",
        "outputId": "c79abf1a-2cba-4850-8fc5-195c3f7dad3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_df=0.80, max_features=200000,\n",
        "                                 min_df=0.07, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "\n",
        "\n",
        "counts = vectorizer.fit_transform(data['headline'])\n",
        "kmeans = KMeans(n_clusters=9, init='k-means++', max_iter=100, n_init=1)\n",
        "    \n",
        "transformed = kmeans.fit_transform(counts)\n",
        "data['cluster_num'] = kmeans.fit_predict(counts)\n",
        "\n",
        "label = kmeans.labels_\n",
        "sil_coeff = silhouette_score(counts, label, metric='euclidean')\n",
        "print(sil_coeff)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9m7i630_u6QO",
        "colab_type": "code",
        "outputId": "808c68b8-3553-4a73-ce3f-eb634f348fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "print(transformed)\n",
        "np.savetxt('gdrive/My Drive/Colab Notebooks/prediction2.txt', np.array(transformed))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.         1.         ... 1.         1.         1.        ]\n",
            " [1.         0.         1.         ... 1.         1.         1.        ]\n",
            " [0.         1.         1.41421356 ... 1.41421356 0.95224851 0.50171421]\n",
            " ...\n",
            " [0.         1.         1.41421356 ... 1.41421356 0.95224851 0.50171421]\n",
            " [1.         0.         1.         ... 1.         1.         1.        ]\n",
            " [1.         0.         1.         ... 1.         1.         1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "73lK_AyrV1xQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with open('gdrive/My Drive/Colab Notebooks/prediction2.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    writer.writerow([\"id\", \"cluster\"])\n",
        "    for index, row in data.iterrows():\n",
        "        Y = vectorizer.transform([row['headline']])\n",
        "        prediction = kmeans.predict(Y)\n",
        "#         print(prediction)\n",
        "        writer.writerow([row['id'], prediction[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gCo9EovWqVY2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}